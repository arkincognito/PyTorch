{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Pytorch_Multivariable_Linear_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZ/9xRWZeqOMZOEbwfPQ9G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkincognito/PyTorch/blob/main/03_Pytorch_Multivariable_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju_1XGzoVi3e"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nWCdSZhY8Tm"
      },
      "source": [
        "# Define training dataset\n",
        ">```\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTgHFkrhZE8B"
      },
      "source": [
        "# Weights and Biases\n",
        "\n",
        "Hypothesis:\n",
        "$$ H(x)= x\\times W + b = \\begin{pmatrix}\n",
        "x_1 & x_2 & x_3 & \\cdots & x_n\\\\\n",
        "\\end{pmatrix}\n",
        "\\times\n",
        "\\begin{pmatrix}\n",
        "w_1\\\\\n",
        "w_2\\\\\n",
        "w_3\\\\\n",
        "\\vdots\\\\\n",
        "w_n\n",
        "\\end{pmatrix} + b$$\n",
        "```\n",
        "W = torch.zeros((3,1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "hypothesis = x_train.mm(W) + b\n",
        "```\n",
        "```requires_grad=True``` indicates that the parameter will be trained.<br>\n",
        "Last line is identical to ```hypothesis = x_train.matmul(W) + b```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et6s1V-QZTkV"
      },
      "source": [
        "#Optimizer and Learning Rate\n",
        "We'll use SGD and Learning Rate of 1e-5.\n",
        "```\n",
        "optimizer = torch.optim.SGD([W, b], lr = 1e-5)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nuur2x-5mil"
      },
      "source": [
        "# Cost(MSE)\n",
        "\n",
        "$$cost(W, b) = {1\\over m}\\sum \\limits _{i=1} ^m (H(x^{(i)}) - y^{(i)})^2\n",
        "$$\n",
        "\n",
        "```\n",
        "cost = torch.mean((hypothesis - y_train)**2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilr2yCLqKSbm"
      },
      "source": [
        "# Update Weight and Bias\n",
        "\n",
        "$\\Delta w_j = {\\partial cost \\over {\\partial w_j}}\n",
        "= {2\\over m}\\sum \\limits _{i=1} ^m (H(x^{(i)})x_j - y^{(i)})$<br>\n",
        "\n",
        "$w_j : = w_j - lr \\Delta w_j$\n",
        "\n",
        "$\\Delta b = {\\partial cost \\over {\\partial b}}\n",
        "= {2\\over m}\\sum \\limits _{i=1} ^m (H(x^{(i)}) - y^{(i)})$<br>\n",
        "\n",
        "$b : = b - lr \\Delta b$\n",
        "\n",
        "Initialize all the gradients to zero:\n",
        "```\n",
        "optimizer.zero_grad()\n",
        "```\n",
        "\n",
        "Backward Propagation:\n",
        "\n",
        "```\n",
        "cost.backward()\n",
        "```\n",
        "\n",
        "Update:\n",
        "```\n",
        "optimizer.step()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQL6-PRMV2Gg",
        "outputId": "481d551e-3bba-4c31-b5b5-548dead7fc20"
      },
      "source": [
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "\n",
        "W = torch.zeros((3,1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "optimizer = torch.optim.SGD([W, b], lr = 1e-5)\n",
        "print(f'Train data:\\t\\t\\t{y_train.squeeze().detach()}')\n",
        "nb_epoch = 1000\n",
        "for epoch in range(nb_epoch):\n",
        "  # Calculate H(x)\n",
        "  hypothesis = x_train.mm(W) + b\n",
        "  # Calculate cost\n",
        "  cost = torch.mean((hypothesis - y_train)**2)\n",
        "  # Update Weights and Bias\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 100 == 99:\n",
        "    print(f'epoch: {epoch:4d}\\t|hypothesis: {hypothesis.squeeze().detach()}\\t|cost: {cost.item():.4f}')\n",
        "print('train finished')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data:\t\t\ttensor([152., 185., 180., 196., 142.])\n",
            "epoch:   99\t|hypothesis: tensor([152.7695, 183.6982, 180.9592, 197.0628, 140.1332])\t|cost: 1.5643\n",
            "epoch:  199\t|hypothesis: tensor([152.7277, 183.7271, 180.9466, 197.0518, 140.1727])\t|cost: 1.4982\n",
            "epoch:  299\t|hypothesis: tensor([152.6870, 183.7551, 180.9344, 197.0410, 140.2112])\t|cost: 1.4356\n",
            "epoch:  399\t|hypothesis: tensor([152.6474, 183.7825, 180.9225, 197.0305, 140.2487])\t|cost: 1.3763\n",
            "epoch:  499\t|hypothesis: tensor([152.6089, 183.8091, 180.9109, 197.0202, 140.2852])\t|cost: 1.3200\n",
            "epoch:  599\t|hypothesis: tensor([152.5714, 183.8349, 180.8997, 197.0102, 140.3208])\t|cost: 1.2667\n",
            "epoch:  699\t|hypothesis: tensor([152.5350, 183.8601, 180.8888, 197.0004, 140.3554])\t|cost: 1.2162\n",
            "epoch:  799\t|hypothesis: tensor([152.4995, 183.8846, 180.8781, 196.9908, 140.3891])\t|cost: 1.1683\n",
            "epoch:  899\t|hypothesis: tensor([152.4651, 183.9085, 180.8678, 196.9815, 140.4220])\t|cost: 1.1229\n",
            "epoch:  999\t|hypothesis: tensor([152.4315, 183.9316, 180.8578, 196.9724, 140.4540])\t|cost: 1.0798\n",
            "train finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L05qItrHWXG5",
        "outputId": "ff8ceba3-8c06-44d1-9bf1-c47e51931feb"
      },
      "source": [
        "x_test = torch.FloatTensor([72, 81, 76]).unsqueeze(0)\n",
        "print(f'W: {W}\\n b: {b}')\n",
        "print(f'y_test: {x_test.mm(W) + b}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W: tensor([[0.7179],\n",
            "        [0.6126],\n",
            "        [0.6801]], requires_grad=True)\n",
            " b: tensor([0.0092], requires_grad=True)\n",
            "y_test: tensor([[153.0060]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mwLfbt0TIkL"
      },
      "source": [
        "# nn.Module implementation\n",
        "\n",
        "Instead of designing model from the scratch everytime, we could inherit the nn.Module class to build our model.<br>\n",
        "Set the input and output for the linear layer.<br>\n",
        "Write ```forward()```<br>\n",
        "```backward()``` will automatically propagate from cost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3Fpox8sYaow"
      },
      "source": [
        "class MultivariateLinearRegressionModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(3, 1) ## Designate input and output dimensions.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylJE56GBnFJN"
      },
      "source": [
        "# nn.functional implementation\n",
        "\n",
        "We can use predefined cost function provided in nn.functional.<br>\n",
        "Changing cost function becomes much easier this way.\n",
        "\n",
        "Instead of explicitly writing MSE Loss,\n",
        "```\n",
        "cost = torch.mean((hypothesis - y_train)**2)\n",
        "```\n",
        "implement the mse_loss method provided in nn.functional.\n",
        "```\n",
        "cost = torch.nn.functional.mse_loss(hypothesis, y_train)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZWch2GVTXuf",
        "outputId": "203e8ca0-81fc-48e3-a1ab-24c1ef6c695f"
      },
      "source": [
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "\n",
        "model = MultivariateLinearRegressionModel()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)\n",
        "print(f'Train data:\\t\\t\\t{y_train.squeeze().detach()}')\n",
        "nb_epoch = 1000\n",
        "for epoch in range(nb_epoch):\n",
        "\n",
        "  # Calculate H(x)\n",
        "  hypothesis = model(x_train)\n",
        "  # Calculate cost\n",
        "  cost = torch.nn.functional.mse_loss(hypothesis, y_train)\n",
        "  \n",
        "  # Initialize all the gradients to zero\n",
        "  optimizer.zero_grad()\n",
        "  # Backward Propagation\n",
        "  cost.backward()\n",
        "  # Update\n",
        "  optimizer.step()\n",
        "  if epoch % 100 == 99:\n",
        "    print(f'epoch: {epoch:4d}\\t|hypothesis: {hypothesis.squeeze().detach()}\\t|cost: {cost.item():.4f}')\n",
        "print('train finished')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data:\t\t\ttensor([152., 185., 180., 196., 142.])\n",
            "epoch:   99\t|hypothesis: tensor([154.0974, 182.8673, 181.5178, 196.6237, 139.6704])\t|cost: 3.4134\n",
            "epoch:  199\t|hypothesis: tensor([154.0283, 182.9146, 181.4966, 196.6084, 139.7325])\t|cost: 3.2428\n",
            "epoch:  299\t|hypothesis: tensor([153.9610, 182.9607, 181.4760, 196.5935, 139.7930])\t|cost: 3.0812\n",
            "epoch:  399\t|hypothesis: tensor([153.8956, 183.0055, 181.4559, 196.5790, 139.8519])\t|cost: 2.9281\n",
            "epoch:  499\t|hypothesis: tensor([153.8319, 183.0492, 181.4364, 196.5649, 139.9092])\t|cost: 2.7831\n",
            "epoch:  599\t|hypothesis: tensor([153.7700, 183.0917, 181.4174, 196.5511, 139.9649])\t|cost: 2.6458\n",
            "epoch:  699\t|hypothesis: tensor([153.7096, 183.1330, 181.3989, 196.5378, 140.0192])\t|cost: 2.5156\n",
            "epoch:  799\t|hypothesis: tensor([153.6509, 183.1733, 181.3810, 196.5247, 140.0720])\t|cost: 2.3924\n",
            "epoch:  899\t|hypothesis: tensor([153.5938, 183.2124, 181.3634, 196.5121, 140.1234])\t|cost: 2.2756\n",
            "epoch:  999\t|hypothesis: tensor([153.5382, 183.2506, 181.3464, 196.4998, 140.1734])\t|cost: 2.1651\n",
            "train finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX7maKk-T0Nf",
        "outputId": "c685b484-2d07-4dc6-94f6-1dbc62eff9f0"
      },
      "source": [
        "x_test = torch.FloatTensor([72, 81, 76]).unsqueeze(0)\n",
        "w, b = model.parameters()\n",
        "print(f'w: {w},\\nb:{b}')\n",
        "print(f'y_test: {model(x_test).item()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w: Parameter containing:\n",
            "tensor([[0.8476, 0.8337, 0.3385]], requires_grad=True),\n",
            "b:Parameter containing:\n",
            "tensor([-0.4140], requires_grad=True)\n",
            "y_test: 153.8621826171875\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}