{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_Pytorch_Minibatch_Linear_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxkqXCgKe3ng5S4WIZetiZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkincognito/PyTorch/blob/main/04_Pytorch_Minibatch_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju_1XGzoVi3e"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as func\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nWCdSZhY8Tm"
      },
      "source": [
        "# Define training dataset\n",
        ">```\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTgHFkrhZE8B"
      },
      "source": [
        "# Weights and Biases\n",
        "\n",
        "Hypothesis:\n",
        "$ H(x)= x\\times W + b = \\begin{pmatrix}\n",
        "x_1 & x_2 & x_3 & \\cdots & x_n\\\\\n",
        "\\end{pmatrix}\n",
        "\\times\n",
        "\\begin{pmatrix}\n",
        "w_1\\\\\n",
        "w_2\\\\\n",
        "w_3\\\\\n",
        "\\vdots\\\\\n",
        "w_n\n",
        "\\end{pmatrix} + b$\n",
        "```\n",
        "W = torch.zeros((3,1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "hypothesis = x_train.mm(W) + b ## or x_train.matmul(W) + b\n",
        "```\n",
        "\n",
        "requires_grad indicates that the parameter will be trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et6s1V-QZTkV"
      },
      "source": [
        "#Optimizer and Learning Rate\n",
        "We'll use SGD and Learning Rate of 0.01.\n",
        "```\n",
        "optimizer = torch.optim.SGD([W, b], lr = 0.01)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nuur2x-5mil"
      },
      "source": [
        "# Cost(MSE)\n",
        "\n",
        "$\n",
        "cost(W, b) = {1\\over m}\\sum \\limits _{i=1} ^m (H(x^{(i)}) - y^{(i)})^2\n",
        "$\n",
        "\n",
        "```\n",
        "cost = torch.mean((hypothesis - y_train)**2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilr2yCLqKSbm"
      },
      "source": [
        "# Update\n",
        "\n",
        "$\\Delta w_j = {\\partial cost \\over {\\partial w_j}}\n",
        "= {2\\over m}\\sum \\limits _{i=1} ^m (H(x^{(i)})x_j - y^{(i)})$<br>\n",
        "\n",
        "$w_j : = w_j - lr \\Delta w_j$\n",
        "\n",
        "$\\Delta b = {\\partial cost \\over {\\partial b}}\n",
        "= {2\\over m}\\sum \\limits _{i=1} ^m (H(x^{(i)}) - y^{(i)})$<br>\n",
        "\n",
        "$b : = b - lr \\Delta b$\n",
        "\n",
        "\n",
        "Initialize all the gradients to zero:\n",
        "```\n",
        "optimizer.zero_grad()\n",
        "```\n",
        "\n",
        "Backward Propagation:\n",
        "\n",
        "```\n",
        "cost.backward()\n",
        "```\n",
        "\n",
        "Update:\n",
        "```\n",
        "optimizer.step()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mwLfbt0TIkL"
      },
      "source": [
        "# nn.Module implementation\n",
        "\n",
        "Instead of designing model from the scratch everytime, we could inherit the nn.Module class to build our model.<br>\n",
        "Set the input and output for the linear layer.<br>\n",
        "Write ```forward()```.<br>\n",
        "```backward()``` will automatically propagate from cost.\n",
        "\n",
        "```\n",
        "class MultivariateLinearRegressionModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(3, 1) ## Designate input and output dimensions.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylJE56GBnFJN"
      },
      "source": [
        "# nn.functional implementation\n",
        "\n",
        "We can use predefined cost function provided in nn.functional.<br>\n",
        "Changing cost function becomes much easier this way.\n",
        "\n",
        "Instead of explicitly writing MSE Loss,\n",
        "```\n",
        "cost = torch.mean((hypothesis - y_train)**2)\n",
        "```\n",
        "implement the mse_loss method provided in nn.functional.\n",
        "```\n",
        "cost = torch.nn.functional.mse_loss(prediction, y_train)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqpnXVcyM1br"
      },
      "source": [
        "# Minibatch\n",
        "Inherit ```torch.utils.data``` to build dataset.\n",
        "```\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = [[73, 80, 75],\n",
        "                    [93, 88, 93],\n",
        "                    [89, 91, 90],\n",
        "                    [96, 98, 100],\n",
        "                    [73, 66, 70]]\n",
        "    self.y_data = [[152], [185], [180], [196], [142]]\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        " \n",
        "  def __get_item__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx])\n",
        "    y = torch.FloatTensor(self.y_data[idx])\n",
        "    return x, y\n",
        " \n",
        "dataset=CustomDataset()\n",
        "```\n",
        "\n",
        "Feed this dataset into ```DataLoader``` to get batches of the dataset.\n",
        "\n",
        "```\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True,)\n",
        "```\n",
        "\n",
        "```batch_size``` designates the size of each minibatch. Usually a power of 2.<br>\n",
        "When ```shuffle``` is ```True```, shuffle batch data on each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZWch2GVTXuf",
        "outputId": "5cbe65d6-e779-4e09-eb9e-4115cb70d0d5"
      },
      "source": [
        "# Build dataset\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = [[73, 80, 75],\n",
        "                    [93, 88, 93],\n",
        "                    [89, 91, 90],\n",
        "                    [96, 98, 100],\n",
        "                    [73, 66, 70]]\n",
        "    self.y_data = [[152], [185], [180], [196], [142]]\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        " \n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx])\n",
        "    y = torch.FloatTensor(self.y_data[idx])\n",
        "    return x, y\n",
        "\n",
        "# Build 1 layer linear regression model\n",
        "class MultivariateLinearRegressionModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(3, 1) ## Designate input and output dimensions.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        " \n",
        "dataset=CustomDataset()\n",
        "\n",
        "# Take a look at the dataset\n",
        "x, y = dataset.__getitem__(slice(0,len(dataset)))\n",
        "print(f'Train data x: {x.squeeze()}')\n",
        "print(f'Train data y: {y.squeeze()}')\n",
        "\n",
        "# Feed dataset through dataloader\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True,)\n",
        "\n",
        "# Build model feed parameters to update to SGD optimizer\n",
        "model = MultivariateLinearRegressionModel()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
        "\n",
        "nb_epoch = 20\n",
        "for epoch in range(nb_epoch):\n",
        "  # Load data in minibatches\n",
        "  for batch_idx, samples in enumerate(dataloader):\n",
        "    x_train, y_train = samples\n",
        "    # Calculate H(x)\n",
        "    prediction = model(x_train)\n",
        "    # Calculate cost\n",
        "    cost = torch.nn.functional.mse_loss(prediction, y_train)\n",
        "    # Update weights and bias\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    # Show progress\n",
        "    print('Epoch: {:4d}/{}\\tBatch{}/{}\\tcost: {}'.\n",
        "          format(epoch+1, nb_epoch, batch_idx+1, len(dataloader), cost.item()))\n",
        "print('train finished')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data x: tensor([[ 73.,  80.,  75.],\n",
            "        [ 93.,  88.,  93.],\n",
            "        [ 89.,  91.,  90.],\n",
            "        [ 96.,  98., 100.],\n",
            "        [ 73.,  66.,  70.]])\n",
            "Train data y: tensor([152., 185., 180., 196., 142.])\n",
            "Epoch:    1/20\tBatch1/3\tcost: 23864.25\n",
            "Epoch:    1/20\tBatch2/3\tcost: 7236.0439453125\n",
            "Epoch:    1/20\tBatch3/3\tcost: 1068.4761962890625\n",
            "Epoch:    2/20\tBatch1/3\tcost: 730.6547241210938\n",
            "Epoch:    2/20\tBatch2/3\tcost: 368.2947998046875\n",
            "Epoch:    2/20\tBatch3/3\tcost: 97.2129898071289\n",
            "Epoch:    3/20\tBatch1/3\tcost: 23.290531158447266\n",
            "Epoch:    3/20\tBatch2/3\tcost: 5.450342178344727\n",
            "Epoch:    3/20\tBatch3/3\tcost: 8.140374183654785\n",
            "Epoch:    4/20\tBatch1/3\tcost: 1.7445882558822632\n",
            "Epoch:    4/20\tBatch2/3\tcost: 0.1987265795469284\n",
            "Epoch:    4/20\tBatch3/3\tcost: 0.06881655752658844\n",
            "Epoch:    5/20\tBatch1/3\tcost: 1.171057939529419\n",
            "Epoch:    5/20\tBatch2/3\tcost: 0.3484415113925934\n",
            "Epoch:    5/20\tBatch3/3\tcost: 0.11361392587423325\n",
            "Epoch:    6/20\tBatch1/3\tcost: 0.01632852852344513\n",
            "Epoch:    6/20\tBatch2/3\tcost: 1.4714162349700928\n",
            "Epoch:    6/20\tBatch3/3\tcost: 9.313225746154785e-08\n",
            "Epoch:    7/20\tBatch1/3\tcost: 0.096734419465065\n",
            "Epoch:    7/20\tBatch2/3\tcost: 1.3779354095458984\n",
            "Epoch:    7/20\tBatch3/3\tcost: 0.00515636894851923\n",
            "Epoch:    8/20\tBatch1/3\tcost: 1.2404941320419312\n",
            "Epoch:    8/20\tBatch2/3\tcost: 0.17300643026828766\n",
            "Epoch:    8/20\tBatch3/3\tcost: 0.059232696890830994\n",
            "Epoch:    9/20\tBatch1/3\tcost: 1.1899291276931763\n",
            "Epoch:    9/20\tBatch2/3\tcost: 0.15633803606033325\n",
            "Epoch:    9/20\tBatch3/3\tcost: 0.4428657293319702\n",
            "Epoch:   10/20\tBatch1/3\tcost: 0.04603659734129906\n",
            "Epoch:   10/20\tBatch2/3\tcost: 1.4319998025894165\n",
            "Epoch:   10/20\tBatch3/3\tcost: 0.07147561758756638\n",
            "Epoch:   11/20\tBatch1/3\tcost: 1.196946382522583\n",
            "Epoch:   11/20\tBatch2/3\tcost: 0.38181057572364807\n",
            "Epoch:   11/20\tBatch3/3\tcost: 0.011962890625\n",
            "Epoch:   12/20\tBatch1/3\tcost: 0.25679731369018555\n",
            "Epoch:   12/20\tBatch2/3\tcost: 1.3405760526657104\n",
            "Epoch:   12/20\tBatch3/3\tcost: 0.010772422887384892\n",
            "Epoch:   13/20\tBatch1/3\tcost: 0.08335567265748978\n",
            "Epoch:   13/20\tBatch2/3\tcost: 1.3044590950012207\n",
            "Epoch:   13/20\tBatch3/3\tcost: 0.1322394758462906\n",
            "Epoch:   14/20\tBatch1/3\tcost: 0.04903717339038849\n",
            "Epoch:   14/20\tBatch2/3\tcost: 1.3410829305648804\n",
            "Epoch:   14/20\tBatch3/3\tcost: 0.10940533131361008\n",
            "Epoch:   15/20\tBatch1/3\tcost: 1.1211645603179932\n",
            "Epoch:   15/20\tBatch2/3\tcost: 0.4995618462562561\n",
            "Epoch:   15/20\tBatch3/3\tcost: 0.11134180426597595\n",
            "Epoch:   16/20\tBatch1/3\tcost: 0.1359204351902008\n",
            "Epoch:   16/20\tBatch2/3\tcost: 1.4249471426010132\n",
            "Epoch:   16/20\tBatch3/3\tcost: 0.0009444067254662514\n",
            "Epoch:   17/20\tBatch1/3\tcost: 0.07147109508514404\n",
            "Epoch:   17/20\tBatch2/3\tcost: 1.310024619102478\n",
            "Epoch:   17/20\tBatch3/3\tcost: 0.12396124005317688\n",
            "Epoch:   18/20\tBatch1/3\tcost: 1.1460366249084473\n",
            "Epoch:   18/20\tBatch2/3\tcost: 0.33552053570747375\n",
            "Epoch:   18/20\tBatch3/3\tcost: 0.04319110885262489\n",
            "Epoch:   19/20\tBatch1/3\tcost: 0.18683184683322906\n",
            "Epoch:   19/20\tBatch2/3\tcost: 1.4124261140823364\n",
            "Epoch:   19/20\tBatch3/3\tcost: 0.004862644709646702\n",
            "Epoch:   20/20\tBatch1/3\tcost: 0.3143300712108612\n",
            "Epoch:   20/20\tBatch2/3\tcost: 1.2974048852920532\n",
            "Epoch:   20/20\tBatch3/3\tcost: 0.18250010907649994\n",
            "train finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX7maKk-T0Nf",
        "outputId": "490d55ce-7460-4a28-a50f-03e64bd39821"
      },
      "source": [
        "x_test = torch.FloatTensor([72, 81, 76]).unsqueeze(0)\n",
        "w, b = model.parameters()\n",
        "print(f'w: {w},\\nb:{b}')\n",
        "print(f'y_test: {model(x_test).item()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w: Parameter containing:\n",
            "tensor([[1.1188, 0.3812, 0.5132]], requires_grad=True),\n",
            "b:Parameter containing:\n",
            "tensor([-0.1042], requires_grad=True)\n",
            "y_test: 150.3267364501953\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}