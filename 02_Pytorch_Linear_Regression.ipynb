{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Pytorch_Linear_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXVCxDkaTo5CBwm4OkBvoj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkincognito/PyTorch/blob/main/02_Pytorch_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju_1XGzoVi3e"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nWCdSZhY8Tm"
      },
      "source": [
        "# Define training dataset\n",
        ">```\n",
        "x_train = torch.FloatTensor([1,2,3])\n",
        "y_train = torch.FloatTensor([2,4,6])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTgHFkrhZE8B"
      },
      "source": [
        "# Weights and Biases\n",
        "\n",
        "Hypothesis:\n",
        "$ H(x)= x \\times W + b$\n",
        "```\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "hypothesis = x_train * W + b\n",
        "```\n",
        "\n",
        "requires_grad indicates that the parameter will be trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et6s1V-QZTkV"
      },
      "source": [
        "#Optimizer and Learning Rate\n",
        "We'll use SGD and Learning Rate of 0.01.\n",
        "```\n",
        "optimizer = torch.optim.SGD([W, b], lr = 0.01)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nuur2x-5mil"
      },
      "source": [
        "# Cost(MSE)\n",
        "\n",
        "$\n",
        "cost(W, b) = {1\\over m}\\sum \\limits _{i=1} ^m (H(x^{(i)}) - y^{(i)})^2\n",
        "$\n",
        "\n",
        "```\n",
        "cost = torch.mean((hypothesis - y_train)**2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilr2yCLqKSbm"
      },
      "source": [
        "# Update\n",
        "\n",
        "$\\Delta w_j = {\\partial cost \\over {\\partial w_j}}\n",
        "= {2\\over m}\\sum \\limits _{i=1} ^m (H(x^{(i)})x_j - y^{(i)})$<br>\n",
        "\n",
        "$w_j : = w_j - lr \\Delta w_j$\n",
        "\n",
        "$\\Delta b = {\\partial cost \\over {\\partial b}}\n",
        "= {2\\over m}\\sum \\limits _{i=1} ^m (H(x^{(i)}) - y^{(i)})$<br>\n",
        "\n",
        "$b : = b - lr \\Delta b$\n",
        "\n",
        "\n",
        "\n",
        "Initialize all the gradients to zero:\n",
        "```\n",
        "optimizer.zero_grad()\n",
        "```\n",
        "\n",
        "Backward Propagation:\n",
        "\n",
        "```\n",
        "cost.backward()\n",
        "```\n",
        "\n",
        "Update:\n",
        "```\n",
        "optimizer.step()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQL6-PRMV2Gg",
        "outputId": "fd808f69-fbf1-4faf-f7d8-87f60ae89f1c"
      },
      "source": [
        "x_train = torch.FloatTensor([1,2,3])\n",
        "y_train = torch.FloatTensor([3,5,7])\n",
        "\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "optimizer = torch.optim.SGD([W, b], lr = 0.01)\n",
        "\n",
        "nb_epoch = 1000\n",
        "for epoch in range(nb_epoch):\n",
        "  hypothesis = x_train * W + b\n",
        "  cost = torch.mean((hypothesis - y_train)**2)\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 50 == 0:\n",
        "    print(f'epoch: {epoch}\\t|W: {W}\\t|b: {b}\\t|cost: {cost}')\n",
        "print('train finished')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0\t|W: tensor([0.2267], requires_grad=True)\t|b: tensor([0.1000], requires_grad=True)\t|cost: 27.66666603088379\n",
            "epoch: 50\t|W: tensor([2.0341], requires_grad=True)\t|b: tensor([0.9087], requires_grad=True)\t|cost: 0.0013602717081084847\n",
            "epoch: 100\t|W: tensor([2.0347], requires_grad=True)\t|b: tensor([0.9210], requires_grad=True)\t|cost: 0.0008990611531771719\n",
            "epoch: 150\t|W: tensor([2.0308], requires_grad=True)\t|b: tensor([0.9300], requires_grad=True)\t|cost: 0.0007067490951158106\n",
            "epoch: 200\t|W: tensor([2.0273], requires_grad=True)\t|b: tensor([0.9379], requires_grad=True)\t|cost: 0.000555570179130882\n",
            "epoch: 250\t|W: tensor([2.0242], requires_grad=True)\t|b: tensor([0.9450], requires_grad=True)\t|cost: 0.00043672314495779574\n",
            "epoch: 300\t|W: tensor([2.0215], requires_grad=True)\t|b: tensor([0.9512], requires_grad=True)\t|cost: 0.00034330846392549574\n",
            "epoch: 350\t|W: tensor([2.0190], requires_grad=True)\t|b: tensor([0.9567], requires_grad=True)\t|cost: 0.0002698726311791688\n",
            "epoch: 400\t|W: tensor([2.0169], requires_grad=True)\t|b: tensor([0.9616], requires_grad=True)\t|cost: 0.00021214706066530198\n",
            "epoch: 450\t|W: tensor([2.0150], requires_grad=True)\t|b: tensor([0.9660], requires_grad=True)\t|cost: 0.00016676487575750798\n",
            "epoch: 500\t|W: tensor([2.0133], requires_grad=True)\t|b: tensor([0.9698], requires_grad=True)\t|cost: 0.00013109373685438186\n",
            "epoch: 550\t|W: tensor([2.0118], requires_grad=True)\t|b: tensor([0.9733], requires_grad=True)\t|cost: 0.00010305390605935827\n",
            "epoch: 600\t|W: tensor([2.0104], requires_grad=True)\t|b: tensor([0.9763], requires_grad=True)\t|cost: 8.100931881926954e-05\n",
            "epoch: 650\t|W: tensor([2.0092], requires_grad=True)\t|b: tensor([0.9790], requires_grad=True)\t|cost: 6.368140020640567e-05\n",
            "epoch: 700\t|W: tensor([2.0082], requires_grad=True)\t|b: tensor([0.9814], requires_grad=True)\t|cost: 5.0055954488925636e-05\n",
            "epoch: 750\t|W: tensor([2.0073], requires_grad=True)\t|b: tensor([0.9835], requires_grad=True)\t|cost: 3.934838605346158e-05\n",
            "epoch: 800\t|W: tensor([2.0064], requires_grad=True)\t|b: tensor([0.9854], requires_grad=True)\t|cost: 3.093302439083345e-05\n",
            "epoch: 850\t|W: tensor([2.0057], requires_grad=True)\t|b: tensor([0.9870], requires_grad=True)\t|cost: 2.431707071082201e-05\n",
            "epoch: 900\t|W: tensor([2.0051], requires_grad=True)\t|b: tensor([0.9885], requires_grad=True)\t|cost: 1.911578692670446e-05\n",
            "epoch: 950\t|W: tensor([2.0045], requires_grad=True)\t|b: tensor([0.9898], requires_grad=True)\t|cost: 1.5026232176751364e-05\n",
            "train finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L05qItrHWXG5",
        "outputId": "2143992e-a885-47b4-b841-2125f3ee4a74"
      },
      "source": [
        "print(f'W: {W}\\n b: {b}')\n",
        "print(f'y(5): {5 * W + b}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W: tensor([2.0040], requires_grad=True)\n",
            " b: tensor([0.9909], requires_grad=True)\n",
            "y(5): tensor([11.0109], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}