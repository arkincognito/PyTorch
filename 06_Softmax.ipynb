{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_Softmax.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPthicg8eegapHrRCtuh62n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkincognito/PyTorch/blob/main/06_Softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuYp0ivhbs_z"
      },
      "source": [
        "# Softmax\n",
        "\n",
        "In case of Multiclass Classification Problem:<br>\n",
        "\n",
        "$$ P(class = i) = {e^{h_i(x)}\\over \\sum_j {e^{h(x)}}} $$\n",
        "\n",
        "Since softmax values represent the probability of class being i, the sum of probabilities of all classes should be 1.<br>\n",
        "\n",
        "$$ \\sum _i P(class = i) = 1$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8AeWsTNZdie",
        "outputId": "b672d1c9-9fb5-4a47-9e53-5b4da3411262"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8bc8489b40>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eANRIi3B2CL8",
        "outputId": "bc6d11f6-5f66-4f16-ddb4-8e7f83a6f2c9"
      },
      "source": [
        "z = torch.FloatTensor([1,2,3])\n",
        "hypothesis = F.softmax(z, dim=0)\n",
        "print(hypothesis)\n",
        "print(hypothesis.sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.0900, 0.2447, 0.6652])\n",
            "tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I8v4d19N-r7"
      },
      "source": [
        "# Cross Entropy Loss\n",
        "Cross Entropy Loss:\n",
        "$$L = {1\\over N} \\sum -y\\log(\\hat y)$$\n",
        "where $y$ is the encoded value of the actual category, and $\\hat y$ is the softmax value of the predicted category"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL6asd9LPZYi"
      },
      "source": [
        "Let's make a quick example of the softmax problem.\n",
        "Let's make a 5 class classification problem with 3 data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfMdaeyt2L3O",
        "outputId": "638925f4-e586-4e96-eb50-db64e67d8a6b"
      },
      "source": [
        "z = torch.rand(3,5, requires_grad=True)\n",
        "hypothesis = F.softmax(z, dim=1)\n",
        "print(hypothesis)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
            "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
            "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmruq7BhPEsN",
        "outputId": "47b71fbf-3cef-480f-a07a-3d23f02e92ca"
      },
      "source": [
        "y = torch.randint(5,(3,)).long()\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 2, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9slTtLsPPk3",
        "outputId": "df1a7515-5377-43c5-9c08-223aa180237d"
      },
      "source": [
        "y_one_hot = torch.zeros_like(hypothesis)\n",
        "#scatter_(dimension to follow, y.unsqueeze(1), filling in value)\n",
        "y_one_hot.scatter_(1, y.unsqueeze(1), 1)\n",
        "print(y_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sAmbSCUQON8",
        "outputId": "28f57f7f-2f81-4610-dffe-61356aa1a0f8"
      },
      "source": [
        "cost = (-y_one_hot * torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()\n",
        "print(cost)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.4689, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL60adZAROJ4"
      },
      "source": [
        "Instead of ```torch.log(F.softmax(z, dim=1))```, we can use ```F.log_softmax(hypothesis)```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AneBjnhQwap",
        "outputId": "f6d704c5-6100-4fb5-ed4e-79e42d8b9f70"
      },
      "source": [
        "cost = (-y_one_hot * F.log_softmax(z, dim=1)).sum(dim=1).mean()\n",
        "print(cost)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.4689, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXq4m3A4RelL",
        "outputId": "79f35745-7897-422c-f926-0b6a636a0763"
      },
      "source": [
        "# Negative Log Likelihood Loss nll_loss()\n",
        "F.nll_loss(F.log_softmax(z, dim=1), y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.4689, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AchpaJ_VOZg"
      },
      "source": [
        "#F.cross_entropy()\n",
        "F.cross_entropy() combines nll_loss and log_softmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooNatpnSUKnE",
        "outputId": "a25b23b3-9db2-42c8-c502-8212f18a9846"
      },
      "source": [
        "F.cross_entropy(z, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.4689, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3pNjP3ulUsG"
      },
      "source": [
        "Let's try 4 parameter 3 class multiclassification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLlz9FLIlTkr"
      },
      "source": [
        "x_train = [[1, 2, 1, 1],\n",
        "           [2, 1, 3, 2],\n",
        "           [3, 1, 3, 4],\n",
        "           [4, 1, 5, 5],\n",
        "           [1, 7, 5, 5],\n",
        "           [1, 2, 5, 6],\n",
        "           [1, 6, 6, 6],\n",
        "           [1, 7, 7, 7]]\n",
        "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
        "x_train = torch.FloatTensor(x_train)\n",
        "y_train = torch.LongTensor(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH_Y-7bJk3ZE"
      },
      "source": [
        "# Implementing nn.Module()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuO9B3Cek-j_"
      },
      "source": [
        "class SoftmaxClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # 4 input parameter, 3 classes\n",
        "    self.linear = nn.Linear(4,3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_Uj2uvdVa-p",
        "outputId": "1318a896-2f51-4f42-d0d5-45eb5a8201d1"
      },
      "source": [
        "model = SoftmaxClassifier()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.08)\n",
        "nb_epoch = 100\n",
        "for epoch in range(nb_epoch+1):\n",
        "  # Calculate z\n",
        "  z = model(x_train)\n",
        "  # Calculate cost\n",
        "  cost = F.cross_entropy(z, y_train)\n",
        "\n",
        "  # Initialize all the gradients to zero\n",
        "  optimizer.zero_grad()\n",
        "  # Backward Propagation\n",
        "  cost.backward()\n",
        "  # Update\n",
        "  optimizer.step()\n",
        "  if epoch % 10 == 0:\n",
        "    # Take Argmax of softmax to predict the class\n",
        "    prediction = F.softmax(z, dim=1).max(dim=1)[1]\n",
        "    accuracy = (prediction == y_train).float().mean()\n",
        "    print(f'Epoch: {epoch:4d}\\t|accuracy: {accuracy:.4f}\\t|cost: {cost.item():.4f}')\n",
        "print('train finished')\n",
        "print(f'Prediction:\\t{prediction}')\n",
        "print(f'Actual:\\t\\t{y_train}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0\t|accuracy: 0.2500\t|cost: 1.5089\n",
            "Epoch:   10\t|accuracy: 0.5000\t|cost: 0.8487\n",
            "Epoch:   20\t|accuracy: 0.6250\t|cost: 0.7611\n",
            "Epoch:   30\t|accuracy: 0.7500\t|cost: 0.7082\n",
            "Epoch:   40\t|accuracy: 0.7500\t|cost: 0.6720\n",
            "Epoch:   50\t|accuracy: 0.8750\t|cost: 0.6448\n",
            "Epoch:   60\t|accuracy: 0.8750\t|cost: 0.6231\n",
            "Epoch:   70\t|accuracy: 0.8750\t|cost: 0.6050\n",
            "Epoch:   80\t|accuracy: 0.8750\t|cost: 0.5894\n",
            "Epoch:   90\t|accuracy: 0.8750\t|cost: 0.5758\n",
            "Epoch:  100\t|accuracy: 0.8750\t|cost: 0.5636\n",
            "train finished\n",
            "Prediction:\ttensor([2, 2, 2, 1, 0, 1, 0, 0])\n",
            "Actual:\t\ttensor([2, 2, 2, 1, 1, 1, 0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZdzgPti4KAK"
      },
      "source": [
        "# Summary\n",
        "\n",
        "###Binary Classification Problem\n",
        "- Sigmoid Function\n",
        "- Binary Cross Entropy: $Cost = - {1\\over m} \\sum _{i=1} ^m ylog(H(x^{(i)}) + (1-y)log(H(x^{(i)}))$\n",
        "\n",
        "###Multi Class Classification Problem\n",
        "- Softmax Function\n",
        "- Cross Entropy: $L = {1\\over N} \\sum -y\\log(\\hat y)$\n",
        "\n"
      ]
    }
  ]
}